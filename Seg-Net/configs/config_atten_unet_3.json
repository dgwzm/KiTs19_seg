{
    "training":{
        "max_it":10,
        "arch_type": "List",
        "n_epochs": 150,
        "save_epoch_freq": 1,
        "gamma":0.82,
	"lr_policy": "step",
        "lr_decay_iters": 8,
        "lr_red_factor": 0.1,
        "batchSize": 24,
        "preloadData": false,
        "num_workers" : 4,
        "sampler": "weighted2",
        "bgd_weight_multiplier": 13
    },
    "visualisation":{
        "server": "http://localhost",
        "display_port": 8097,
        "no_html": true,
        "display_winsize": 300,
        "display_id": 2,
        "display_single_pane_ncols": 0,
	"summary_name":"test1",
        "pth": "/home/linda/wzm/Attention-Gated-Networks/src/atten_unet_3/logs"
    },
    "data_path": {
        "List": "/home/linda/wzm/LITS/"
    },
    "augmentation": {
        "List": {
            "patch_size": [208, 272],
            "shift": [0.02,0.02],
            "rotate": 25.0,
            "scale": [0.7,1.3],
            "intensity": [1.0,1.0],
            "random_flip_prob": 0.5
        }
    },
    "model":{
        "type":"seg",
        "criterion":"dice_loss_3",
        "optim":"adam",
        "model_type": "atten_unet",
        "tensor_dim": "2D",
        "nonlocal_mode": "embedded_gaussian",
        "aggregation_mode": "mean",
        "weight":[1, 1, 1],
        "aggregation":"mean",
        "continue_train": false,
        "which_epoch": 0,
	"wh":256,
        "input_nc": 1,
        "output_nc": 3,
        "attention_dsample": "embedded_gaussian",
        "lr_rate": 0.00004,
        "l2_reg_weight": 1e-4,
        "feature_scale": 4,
        "gpu_ids": [1],
	"cut_gpu":"2",
	"use_more_gpu":true,
        "isTrain": true,
        "path_pre_trained_model":"/home/linda/wzm/Attention-Gated-Networks/src/atten_unet_3/090_net_0.39612.pth",
        "checkpoints_dir": "/home/linda/wzm/Attention-Gated-Networks/src/atten_uneti_3/",
        "experiment_name": "atten_unet_3"
    }
}
